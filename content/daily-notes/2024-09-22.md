### 2024-09-22
Spending time at the hsr hacker house today: [shippin' hours 02 by hsrhackerhouse · Luma](https://lu.ma/ylczj5fj?tk=AG5asc)

#### Notes while adding groq llama 3-1-70b support to podscript
The first thing I did at the shipping hours was to add support for llama-3.1-70b via Groq.

[Add support for llama-3.1-70b via groq · deepakjois/podscript@d031623 · GitHub](https://github.com/deepakjois/podscript/commit/d031623779d81bdebe197a699005a48a09c4d4c3)

The change itself was fairly trivial, because I had previously switched to langchaingo to call LLMs. But I did run into a few issues.

* OpenAI made a change to one of their parameters for the new o1 models, renaming `max_tokens` to `max_completion_tokens` effectively deprecating `max_tokens`. The Groq completion API, which is supposed to be compatible with OpenAI doesn't have that change. So the langchain Groq integration, which just uses the internal OpenAI client broke because they seem to have prematurely deprecated `max_tokens` support. I filed a bug w/ them: [Error running groq completion example · Issue #1028 · tmc/langchaingo · GitHub](https://github.com/tmc/langchaingo/issues/1028)
* A funny thing happened with the llama integration - it gave me the transcript with a single `<tr`

