### 2024-09-22
Spending time at the hsr hacker house today: [shippin' hours 02 by hsrhackerhouse · Luma](https://lu.ma/ylczj5fj?tk=AG5asc)

#### Notes while adding groq llama 3-1-70b support to podscript
The first thing I did at the shipping hours was to add support for llama-3.1-70b via Groq.

[Add support for llama-3.1-70b via groq · deepakjois/podscript@d031623 · GitHub](https://github.com/deepakjois/podscript/commit/d031623779d81bdebe197a699005a48a09c4d4c3)

The change itself was fairly trivial, because I had previously switched to langchaingo to call LLMs. But I did run into a few issues.

* OpenAI made a change to one of their parameters for the new o1 models, renaming `max_tokens` to `max_completion_tokens` effectively deprecating `max_to`

